{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1b8683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     430\n",
       "1     421\n",
       "3     330\n",
       "0     308\n",
       "4     216\n",
       "5     154\n",
       "6      90\n",
       "7      55\n",
       "8      30\n",
       "9      27\n",
       "10     25\n",
       "11     11\n",
       "12      7\n",
       "15      1\n",
       "14      1\n",
       "Name: R06, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DailyDataset1 = pd.read_csv('dailydataset.csv')\n",
    "DailyDataset1['R06'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd9f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DailyDataset2 = pd.read_csv('DailyDataset_updated.csv')\n",
    "DailyDataset2['R06'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cefca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DailyDataset = pd.read_csv('DailyDataset.csv')\n",
    "DailyDataset['R06'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af41f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# load the dataset\n",
    "DailyDataset1 = pd.read_csv('DailyDataset_classified.csv')\n",
    "\n",
    "# split the data into features and target\n",
    "X= DailyDataset1[['Temperature', 'Dew Point', 'Wind Speed', 'Humidity', 'Pressure']]\n",
    "y = DailyDataset1['Classification']\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# create the Random Forest model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "# evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmae = np.sqrt(np.mean(np.abs(y_test - y_pred)**3))\n",
    "# print the evaluation metrics\n",
    "print('rf')\n",
    "print('RMSE: {:.2f}'.format(rmse))\n",
    "print('RMAE: {:.2f}'.format(rmae))\n",
    "print('MSE: {:.2f}'.format(mse))\n",
    "print('R2 : {:.2f}'.format(r2))\n",
    "\n",
    "# create the Gradient Boosting Regressor model\n",
    "model1 = GradientBoostingRegressor(random_state=42)\n",
    "# train the model\n",
    "model1.fit(X_train, y_train)\n",
    "# make predictions on the testing data\n",
    "y_pred1 = model1.predict(X_test)\n",
    "# calculate RMSE\n",
    "rmse1 = np.sqrt(mean_squared_error(y_test, y_pred1))\n",
    "# calculate RMAE\n",
    "rmae1 = np.sqrt(np.mean(np.abs(y_pred1 - y_test)))\n",
    "# calculate MSE\n",
    "mse1 = mean_squared_error(y_test, y_pred1)\n",
    "# calculate R2\n",
    "r21 = r2_score(y_test, y_pred1)\n",
    "print('gb')\n",
    "print('RMSE: {:.2f}'.format(rmse1))\n",
    "print('RMAE: {:.2f}'.format(rmae1))\n",
    "print('MSE: {:.2f}'.format(mse1))\n",
    "print('R2 : {:.2f}'.format(r21))\n",
    "\n",
    "# create the Linear Regression model\n",
    "model2 = LinearRegression()\n",
    "# train the model\n",
    "model2.fit(X_train, y_train)\n",
    "# make predictions on the testing data\n",
    "y_pred2 = model.predict(X_test)\n",
    "# evaluate the model\n",
    "mse2 = mean_squared_error(y_test, y_pred2)\n",
    "rmse2 = np.sqrt(mse2)\n",
    "r22 = r2_score(y_test, y_pred2)\n",
    "rmae2 = np.mean(np.abs(y_pred2 - y_test))\n",
    "print('lr')\n",
    "print('RMSE: {:.2f}'.format(rmse2))\n",
    "print('RMAE: {:.2f}'.format(rmae2))\n",
    "print('MSE: {:.2f}'.format(mse2))\n",
    "print('R2: {:.2f}'.format(r22))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c3e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DailyDataset = pd.read_csv('DailyDataset_classified.csv')\n",
    "DailyDataset['Classification'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0218442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"DailyDataset_classified.csv\")  # Replace \"DailyDataset_classified.csv\" with the actual file name or path\n",
    "\n",
    "\n",
    "df['datum'] = pd.to_datetime(df['datum'])\n",
    "\n",
    "# create new columns for year, month, and day\n",
    "df['year'] = df['datum'].dt.year\n",
    "df['month'] = df['datum'].dt.month\n",
    "df['day'] = df['datum'].dt.day\n",
    "\n",
    "\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X= df[['Temperature', 'Dew Point', 'Wind Speed', 'Humidity', 'Pressure','year','day','month']]\n",
    "y = df[\"Classification\"]  # Target is \"Classification\" column\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest Classifier with default hyperparameters\n",
    "rf_classifier = RandomForestClassifier(criterion='entropy',n_estimators=300,max_depth=3)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e914321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "import math\n",
    "\n",
    "# L\n",
    "df = pd.read_csv(\"DailyDataset_classified.csv\")  # Replace \"DailyDataset_classified.csv\" with the actual file name or path\n",
    "\n",
    "\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X= df[['Year','Month','Hour','Day','Temperature','Dew Point','Humidity','Wind','Wind Speed','Pressure','IntCondition']]\n",
    "y = df[\"Classification\"]  # Target is \"Classification\" column\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n",
    "\n",
    "# Create a Random Forest Classifier with default hyperparameters\n",
    "rf_classifier = RandomForestClassifier(criterion='gini',n_estimators=10,max_depth=2,max_features='sqrt')\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = rf_classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88486238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f559d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5999419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb011cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c13571",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb9e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68429cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f1dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b00a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv(\"DailyDataset_classified.csv\")\n",
    "X= df[['Year','Month','Day','Temperature','Dew Point','Humidity','Wind','Wind Speed','Pressure','IntCondition']]\n",
    "y = df[\"Classification\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Define a grid of hyperparameters to search over\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'degree': [1,  3, 4, 5],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object and fit it to the training data\n",
    "grid_search = GridSearchCV(svm_classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data using the best estimator found by GridSearchCV\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8af13b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Classification'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Classification'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDailyDataset_classified.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m X\u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDay\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemperature\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDew Point\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHumidity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWind\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWind Speed\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPressure\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIntCondition\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m---> 13\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClassification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n\u001b[0;32m     16\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Classification'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"DailyDataset_classified.csv\")\n",
    "\n",
    "X= df[['Year','Month','Day','Temperature','Dew Point','Humidity','Wind','Wind Speed','Pressure','IntCondition']]\n",
    "y = df[\"Classification\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the classifiers to be tested and their hyperparameter grids\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": (LogisticRegression(max_iter=5000), \n",
    "                            {'C': [0.01, 0.1, 1.0, 10, 100], 'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "                             'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "                             'class_weight': ['balanced', None]}),\n",
    "    \"Decision Tree\": (DecisionTreeClassifier(), \n",
    "                      {'criterion': ['gini', 'entropy'], 'max_depth': [3, 5, None],\n",
    "                       'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}),\n",
    "    \"Random Forest\": (RandomForestClassifier(), \n",
    "                      {'n_estimators': [50, 100, 150], 'max_depth': [3, 5, None],\n",
    "                       'max_features': ['sqrt', 'log2'], 'min_samples_split': [2, 5, 10],\n",
    "                       'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}),\n",
    "    \"SVM\": (SVC(), \n",
    "            {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto'],\n",
    "             'class_weight': ['balanced', None]}),\n",
    "    \"Gradient Boosting\": (GradientBoostingClassifier(), \n",
    "                          {'n_estimators': [50, 100, 200], 'max_depth': [3, 15, None],\n",
    "                           'max_features': ['sqrt', 'log2'], 'learning_rate': [0.01, 0.1, 1],\n",
    "                           'subsample': [0.5, 0.8, 1.0]})\n",
    "}\n",
    "\n",
    "# Perform cross-validation and hyperparameter tuning for each classifier\n",
    "results = {}\n",
    "for clf_name, (clf, param_grid) in classifiers.items():\n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    best_clf = grid_search.best_estimator_\n",
    "    accuracy_scores = cross_val_score(best_clf, X_train_scaled, y_train, cv=10)\n",
    "    results[clf_name] = {'Best Params': best_clf.get_params(), \n",
    "                         'Mean Accuracy': accuracy_scores.mean(),\n",
    "                         'Accuracy STD': accuracy_scores.std()}\n",
    "\n",
    "# Convert the results dictionary to a pandas DataFrame\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e2397e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Arad', 'Sibiu', 'Rimnicu Vilcea', 'Pitesti', 'Bucharest'], 418)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a23377",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# Define X and y\n",
    "X = df[['Year', 'Month', 'Day', 'Temperature', 'Dew Point', 'Humidity', 'Wind', 'Wind Speed', 'Pressure', 'IntCondition']]\n",
    "y = df['R06']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a dictionary of models to test with their corresponding hyperparameters\n",
    "models = {\n",
    "    'Linear Regression': {\n",
    "        'model': LinearRegression(),\n",
    "        'params': {}\n",
    "    },\n",
    "    'Lasso Regression': {\n",
    "        'model': Lasso(),\n",
    "        'params': {'alpha': [0.1, 1, 10,15,20,40]}\n",
    "    },\n",
    "    'Ridge Regression': {\n",
    "        'model': Ridge(),\n",
    "        'params': {'alpha': [0.1, 1, 10,15,20,40]}\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'model': DecisionTreeRegressor(),\n",
    "        'params': {'max_depth': [5, 10, 15,20,25,30,40,60,100,150]}\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestRegressor(),\n",
    "            'params': {'n_estimators': [50, 100, 150], 'max_depth': [5, 10, 15,20,25,30]}\n",
    "    \n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingRegressor(),\n",
    "        'params': {'n_estimators': [50, 100, 150,], 'max_depth': [5, 10, 15,20,25,30]}\n",
    "    }\n",
    "}\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Loop through the models and perform hyperparameter tuning with GridSearchCV\n",
    "for model_name, model_params in models.items():\n",
    "    print(f'Testing {model_name}...')\n",
    "    grid_search = GridSearchCV(model_params['model'], model_params['params'], cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f'Best parameters: {grid_search.best_params_}')\n",
    "    print(f'Training score: {grid_search.best_score_}')\n",
    "    print(f'Testing score: {grid_search.score(X_test, y_test)}')\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(f'Mean Squared Error (MSE): {mse}')\n",
    "    print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "    print(f'R-squared (R2): {r2}')\n",
    "    print(f'Mean Absolute Error (MAE): {mae}')\n",
    "    residuals = y_test - y_pred\n",
    "    sns.residplot(x=y_test, y=y_pred, data=df)\n",
    "    plt.title(f'{model_name} Residual Plot')\n",
    "    plt.show()\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
